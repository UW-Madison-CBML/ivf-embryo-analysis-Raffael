# Why 128×128 Instead of 512×512?

## Current: 128×128

### Advantages:
1. **Fast Training**
   - 128×128: ~15 min/epoch → 12-15 hours total
   - 512×512: ~2-3 hours/epoch → 100-150 hours total (4-6 days!)

2. **GPU Memory Efficient**
   - 128×128: batch_size=8 fits in 16GB
   - 512×512: batch_size=1-2 only, needs 32-64GB

3. **Sufficient Resolution**
   - 128×128 captures embryo morphology well
   - Most developmental features visible
   - Good for temporal analysis

4. **Faster Iteration**
   - Can test multiple configurations quickly
   - Can train multiple models

## If You Want 512×512

### Required Changes:

1. **Update dataset_ivf.py** (or train.py):
   ```python
   train_dataset = IVFSequenceDataset(index_csv, resize=512, norm="minmax01")
   ```

2. **Update model architecture** (model.py):
   - Current: 128→64→32→16 (3 pooling layers)
   - For 512: 512→256→128→64→32→16 (5 pooling layers)
   - Need to add 2 more pooling layers

3. **Reduce batch size**:
   ```python
   batch_size = 2  # or even 1
   ```

4. **Increase memory**:
   ```
   request_memory = 32GB  # or 64GB
   ```

5. **Much longer training time**:
   - Per epoch: 2-3 hours
   - Total (50 epochs): 100-150 hours (4-6 days)

## Comparison

| Aspect | 128×128 | 512×512 |
|--------|---------|---------|
| Pixels/frame | 16,384 | 262,144 (16×) |
| Batch size | 8 | 1-2 |
| Memory needed | 16GB | 32-64GB |
| Time/epoch | 15 min | 2-3 hours |
| Total time | 12-15 hours | 100-150 hours |
| Resolution | Good | Excellent |

## Recommendation

**Use 128×128** for:
- ✅ Fast training and iteration
- ✅ Efficient GPU usage
- ✅ Sufficient for most analysis
- ✅ Can train multiple experiments

**Use 512×512** if:
- You need to capture fine details
- You have weeks of GPU time
- You've validated 128×128 is insufficient
- You're doing final high-quality training

## Best Practice: Progressive Training

1. Start with 128×128 (current)
2. Train and validate
3. If needed, fine-tune at 256×256
4. Finally train at 512×512 if necessary

This saves time and resources!

